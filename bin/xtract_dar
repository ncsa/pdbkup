#!/usr/bin/bash

set -x

DN=$( dirname $0 )
BASE=$( readlink -e "$DN" )
[[ -z $BASE ]] && exit 1
[[ -d $BASE ]] || exit 1

source $BASE/funcs.sh
source $BASE/read_ini.sh
source $BASE/crudini

PARALLEL=$( which parallel )
[[ -z "$PARALLEL" ]] && die "Unable to find 'parallel'"
[[ -x "$PARALLEL" ]] || die "Parallel program, 'parallel', is not executable"

DAR=$( which dar )
[[ -z "$DAR" ]] && DAR=$BASE/dar
[[ -z "$DAR" ]] && die "Unable to find 'dar' program"
[[ -x "$DAR" ]] || die "Dar binary '$DAR' is not executable"

DAR_OPTS=( --no-warn=all -Q --verbose=all --overwriting-policy Oo )

#Where to extract dar files
RESTOREROOT=/lsst/RESTORETESTTGT
if [[ $# -eq 1 ]] ; then
    RESTOREROOT="$1"
else
    echo "No restore path specified. Using default value: $RESTOREROOT"
    select yn in Yes No; do
        case $yn in
            Yes ) break;;
            * ) exit;;
        esac
    done
fi

# Check for clean restore directory
if [[ -d "$RESTOREROOT" ]] ; then 
    linkcount=$( stat --printf='%h' "$RESTOREROOT" )
    if [[ $linkcount -gt 2 ]] ; then
        echo "Restore directory is not empty. Purge contents of restore directory?"
        select yn in Yes No; do
            case $yn in
                Yes ) break;;
                * ) exit;;
            esac
        done
    fi
    # TODO - make this parallel to reduce runtime
    find "$RESTOREROOT" -delete
fi
mkdir "$RESTOREROOT" || die "Error creating restore dir '$RESTOREROOT'"
crudini --set restore.ini RESTORE ROOT "$RESTOREROOT"


## Clean up any previous restore attempts
rm -f $BASE/*.restore.{log,err,cmd,cmd.log,queue,sqlworker.cmd}


# Create cmd files for each dar extraction (so they can be run in parallel)
alldars=( $( find $BASE -name '*.dar.1.dar' ) )
for darfile in "${alldars[@]}"; do
    fnbase=$( basename $darfile '.dar.1.dar' )
    darbase=$BASE/${fnbase}.dar
    logfile=${darbase}.restore.log
    errfile=${darbase}.restore.err
    cmdfile=${darbase}.restore.cmd
    outfile=${cmdfile}.log
    cat <<ENDHERE >$cmdfile
#!/bin/bash
start=\$( date '+%s' )
echo "STARTTIME: \$start" > $outfile
$DAR ${DAR_OPTS[@]} \\
--extract $darbase \\
--fs-root $RESTOREROOT \\
1> $logfile \\
2> $errfile
end=\$( date '+%s' )
echo "ENDTIME: \$end" >> $outfile
echo "ELAPSED_SECONDS: \$SECONDS" >> $outfile
ENDHERE

done

# Create sqlmaster task queue
Qfile=$BASE/restore.queue
sqlfile=$( echo $Qfile | sed -e 's/\//%2F/g' )
dburl="sqlite3:///$sqlfile/tasks"
find $BASE -name '*restore.cmd' \
| $PARALLEL --sqlmaster $dburl bash {}


# Create sqlworker cmd script
sqlworker_cmdfile=$BASE/restore.sqlworker.cmd
cat <<ENDHERE >$sqlworker_cmdfile
#!/bin/bash
# Dont continue if active parallel or dar processes
parcount=\$( pgrep -f parallel | wc -l )
[[ \$parcount -ne 0 ]] && {
    echo "Existing parallel processes found, exiting."
    exit 1
}
darcount=\$( pgrep -f dar | wc -l )
[[ \$darcount -ne 0 ]] && {
    echo "Existing dar processes found, exiting."
    exit 1
}
# Check parallel version
pver=\$($PARALLEL --version | awk '/^GNU parallel [0-9]+/ {print \$3}')
[[ \$pver -ge 20170222 ]] || {
    echo Missing parallel or version too old
    exit
}
$PARALLEL -j 4 --sqlworker $dburl bash
ENDHERE

set +x 
echo Invoke workers using ...
echo "ssh <WORKER_HOST> 'echo \"bash $sqlworker_cmdfile\" | at now'"

echo "Monitor progress using..."
echo "sqlite3 -column -header $Qfile 'select Seq,Host,Starttime,JobRuntime,Exitval from tasks;'"
